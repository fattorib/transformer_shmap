training:
  max_epochs: 20
  batch_size: 512 # per host batch size
  peak_learning_rate: 6e-4
  warmup_steps: 500
  total_steps: 10000
  end_learning_rate: 6e-5
  weight_decay: 0.1
  gradient_accumulation_steps: 32  
  evaluation_frequency: 500
  maximum_evaluation_steps: 125
  train_context: 1024
  dp: 4 
  mp: 1

model:
  size: "160m"

data:
  corpus_name: ""
  train_bin_path: "data/train.bin"
  validation_bin_path: "data/validation.bin"
  checkpoint_directory: "checkpoints"
  wandb_project: ""
